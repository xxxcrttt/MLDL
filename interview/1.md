## åŸºæœ¬æ¦‚å¿µ

### Q1. The process of solving a ML problem
1. Specified the question: supervised or unsupervised? Classification or Regression?
2. Data collecting and processing 
3. Feature Extraction: construction, selection, combination
4. Model training and evaluation: selection, testing and parameters tuning
5. Model Deployment: edge devices? online? 

### Q2. Loss function æŸå¤±å‡½æ•° -- å•ä¸ªæ ·æœ¬
Loss function calculates the **difference** between the predicted value and the true value about a single sample in ML model.

#### å¸¸ç”¨çš„æŸå¤±å‡½æ•°
* **0-1 loss**: the predicted value != true value ==> 1        
![image](https://user-images.githubusercontent.com/56160038/178030132-f2284c6d-f4c4-4df4-b289-8dc20053d460.png)
* **ç»å¯¹å€¼ absolute loss func**: The robustness is good when there are many abnormal points, but its not convinient to do derivation
```ğ¿(ğ‘¦,ğ‘“(ğ‘¥))=|ğ‘¦âˆ’ğ‘“(ğ‘¥)|```
* **å¹³æ–¹ quadratic loss func**: The square difference, convinient to do derivation, can be optimized by gradient descent, but sensitive to outliers. 
```ğ¿(ğ‘¦,ğ‘“(ğ‘¥))=(ğ‘¦âˆ’ğ‘“(ğ‘¥))^2```
* **å¯¹æ•° logarithms loss func**: maximum likelihood estimation
```ğ¿(ğ‘¦,ğ‘(ğ‘¦|ğ‘¥))=âˆ’logğ‘(ğ‘¦|ğ‘¥)```
* **Huber loss func**: used in robust regression, less sensitive to ourliers, but needs to tune hyperparameters    
![image](https://user-images.githubusercontent.com/56160038/178037719-6c52693f-326b-4fb9-9b29-689326e2ea3b.png)
* **Log-Cosh**: second order differentiable(Newton's method)    
![image](https://user-images.githubusercontent.com/56160038/178039263-e4fcc95b-974b-4cba-a297-6ad237e23cc1.png)
* **Hinge loss**: used in Classification, especially SVM   
![image](https://user-images.githubusercontent.com/56160038/178060505-5bb9b920-21dd-4b4d-950f-6bdab3b83199.png)


### Q3. Cost Function ä»£ä»·å‡½æ•° -- æ•´ä¸ªæ•°æ®é›†
defined over the entire training set, it is the averag eof all sample errors -- the average of all loss function 

* **MAE/ L1**: å¹³å‡ç»å¯¹è¯¯å·® Mean Absolute error, used in Regression model, sum of the absolute value 
* **MSE/ L2**: å‡æ–¹è¯¯å·® Mean Squared Error, sum of the squares of the distance between the predicted value and the true value; 
can evaluate the degree of change in the data. 
* **RMSE**: å‡æ–¹æ ¹è¯¯å·® Root MSE 
When dealing with outliers, L1 is more stable, but its derivative is not continuous, so its less effective;   
L2 is more sensitive to outliers. 
* **äº¤å‰ç†µ Cross-Entry**: Shannon Information Theory, measures the difference information between probability distributions. 
Usually used as cost function in Classification.   
<img src='https://user-images.githubusercontent.com/56160038/178061092-803951b3-a071-41e5-b7ce-a7f335e7baae.png' width='300' align=center />

### Q4. ç»“æ„è¯¯å·®ï¼Œç»éªŒè¯¯å·®ï¼Œæ³›åŒ–è¯¯å·®
* ç»éªŒé£é™©: Empirical Risk/training error: æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¯¯å·®
* æ³›åŒ–è¯¯å·®: Generalization error: æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¯¯å·®
* ç»“æ„é£é™©: Structure Risk: åœ¨ç»éªŒé£é™©ä¸ŠåŠ ä¸Šæ­£åˆ™åŒ–, ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆ

### Q5. æ³›åŒ– -- Generalization
æ¨¡å‹å¯¹äºæœªçŸ¥æ•°æ®çš„é¢„æµ‹èƒ½åŠ›ç§°ä¸ºæ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æµ‹è¯•è¯¯å·®æ¥è¯„ä»·   
The ability of model to predict unknown data is usually called generalization ability. 

### Q6. Metrics è¯„ä¼°æŒ‡æ ‡
<img src='https://user-images.githubusercontent.com/56160038/178064120-2d8c833c-3d8f-4b0e-883a-a937e9d524eb.png' width='300' align=center />

* **Accuracy**: $$ Accuracy = \frac{n_{correct}}{n_{total}} $$
* **Precision**: $$ Precision = \frac{TP}{TP+FP} $$
* **Recall**: $$ Recall = \frac{TP}{TP+FN} $$
* **F1-Score**: harmonic mean of Precision and Recall, $$ {\rm F1} = \frac{2 \times precision \times recall}{precision + recall} $$ 
* **ROC**: FPR to TPR,  $$ FPR = \frac{FP}{N} \ TPR = \frac{TP}{P} $$ 


### Q1. What are the differences between supervised and unsupervised learning?   
* supervised: input data is known and labeled, has a feedback mechanism, such as Logistic Regression, Decision Tree, SVM, Classification
* unsupervised: unlabeled, such as clustering -- K-means clustering, hierarchical clustering

### Q2. How is logistic regression done? 
LR is a generalized linear model, it measures the relationship between the **dependent variable(label)** and one or more **independent variables(features)** by estimating probability 
using its underlying **logistic function(sigmoid)**. 
![image](https://user-images.githubusercontent.com/56160038/178021106-071690b2-5a17-46a5-8dfb-4ec3c7f56f6f.png)

### Q3. Decision Tree
1. input: entire dataset 
2. calculate **entropy** of the target variable
3. calculate **information gain** of all attributes
4. choose the attribute with the highest information gain as the **root**
5. repeat the same procedure on every branch until the decision node is finalized 
ç”Ÿæˆç®—æ³•: ID3, C4.5, C5.0 

### Q4. Random Forest 
It is build up of a number of Decision Trees, 
