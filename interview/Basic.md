## åŸºæœ¬æ¦‚å¿µ

### Q1. The process of solving a ML problem
1. Specified the question: supervised or unsupervised? Classification or Regression?
2. Data collecting and processing 
3. Feature Extraction: construction, selection, combination
4. Model training and evaluation: selection, testing and parameters tuning
5. Model Deployment: edge devices? online? 

### Q2. Loss function æŸå¤±å‡½æ•° -- å•ä¸ªæ ·æœ¬
Loss function calculates the **difference** between the predicted value and the true value about a single sample in ML model.

#### å¸¸ç”¨çš„æŸå¤±å‡½æ•°
* **0-1 loss**: the predicted value != true value ==> 1        
![image](https://user-images.githubusercontent.com/56160038/178030132-f2284c6d-f4c4-4df4-b289-8dc20053d460.png)
* **ç»å¯¹å€¼ absolute loss func**: The robustness is good when there are many abnormal points, but its not convinient to do derivation
```ğ¿(ğ‘¦,ğ‘“(ğ‘¥))=|ğ‘¦âˆ’ğ‘“(ğ‘¥)|```
* **å¹³æ–¹ quadratic loss func**: The square difference, convinient to do derivation, can be optimized by gradient descent, but sensitive to outliers. 
```ğ¿(ğ‘¦,ğ‘“(ğ‘¥))=(ğ‘¦âˆ’ğ‘“(ğ‘¥))^2```
* **å¯¹æ•° logarithms loss func**: maximum likelihood estimation
```ğ¿(ğ‘¦,ğ‘(ğ‘¦|ğ‘¥))=âˆ’logğ‘(ğ‘¦|ğ‘¥)```
* **Huber loss func**: used in robust regression, less sensitive to ourliers, but needs to tune hyperparameters    
![image](https://user-images.githubusercontent.com/56160038/178037719-6c52693f-326b-4fb9-9b29-689326e2ea3b.png)
* **Log-Cosh**: second order differentiable(Newton's method)    
![image](https://user-images.githubusercontent.com/56160038/178039263-e4fcc95b-974b-4cba-a297-6ad237e23cc1.png)
* **Hinge loss**: used in Classification, especially SVM   
![image](https://user-images.githubusercontent.com/56160038/178060505-5bb9b920-21dd-4b4d-950f-6bdab3b83199.png)


### Q3. Cost Function ä»£ä»·å‡½æ•° -- æ•´ä¸ªæ•°æ®é›†
defined over the entire training set, it is the averag eof all sample errors -- the average of all loss function 

* **MAE/ L1**: å¹³å‡ç»å¯¹è¯¯å·® Mean Absolute error, used in Regression model, sum of the absolute value 
* **MSE/ L2**: å‡æ–¹è¯¯å·® Mean Squared Error, sum of the squares of the distance between the predicted value and the true value; 
can evaluate the degree of change in the data. 
* **RMSE**: å‡æ–¹æ ¹è¯¯å·® Root MSE 
When dealing with outliers, L1 is more stable, but its derivative is not continuous, so its less effective;   
L2 is more sensitive to outliers. 
* **äº¤å‰ç†µ Cross-Entry**: Shannon Information Theory, measures the difference information between probability distributions. 
Usually used as cost function in Classification.   
<img src='https://user-images.githubusercontent.com/56160038/178061092-803951b3-a071-41e5-b7ce-a7f335e7baae.png' width='300' align=center />

### Q4. ç»“æ„è¯¯å·®ï¼Œç»éªŒè¯¯å·®ï¼Œæ³›åŒ–è¯¯å·®
* ç»éªŒé£é™©: Empirical Risk/training error: æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¯¯å·®
* æ³›åŒ–è¯¯å·®: Generalization error: æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¯¯å·®
* ç»“æ„é£é™©: Structure Risk: åœ¨ç»éªŒé£é™©ä¸ŠåŠ ä¸Šæ­£åˆ™åŒ–, ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆ

### Q5. æ³›åŒ– -- Generalization
æ¨¡å‹å¯¹äºæœªçŸ¥æ•°æ®çš„é¢„æµ‹èƒ½åŠ›ç§°ä¸ºæ³›åŒ–èƒ½åŠ›ï¼Œé€šè¿‡æµ‹è¯•è¯¯å·®æ¥è¯„ä»·   
The ability of model to predict unknown data is usually called generalization ability. 

### Q6. Metrics è¯„ä¼°æŒ‡æ ‡
**æ··æ·†çŸ©é˜µ Confusion Matrix**:    
<img src='https://user-images.githubusercontent.com/56160038/178064120-2d8c833c-3d8f-4b0e-883a-a937e9d524eb.png' width='300' align=center />

* **Accuracy**: $$ Accuracy = \frac{n_{correct}}{n_{total}} $$ 
* **Precision**: $$ Precision = \frac{TP}{TP+FP} $$
* **Recall**: $$ Recall = \frac{TP}{TP+FN} $$
* **F1-Score**: harmonic mean of Precision and Recall, $$ {\rm F1} = \frac{2 \times precision \times recall}{precision + recall} $$ 
* **ROC**: FPR to TPR,  $$ FPR = \frac{FP}{N} \ TPR = \frac{TP}{P} $$ 
* **AUC**: Area Under Curve, ROCæ›²çº¿ä¸‹çš„é¢ç§¯, å–å€¼ä¸€åŠåœ¨ 0.5~1 ä¹‹é—´
<img src ='https://user-images.githubusercontent.com/56160038/178078824-2393eab7-949e-463b-83b6-5c1c9f521cfd.png' width='300' align=center />

### Q7. Overfitting vs. Underfitting 
#### Trade-off between Bias and Variance 
* **Bias**: æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å·®å¼‚ -- æ¨¡å‹ç®€å•ï¼Œå¯¼è‡´æ¬ æ‹Ÿåˆ
* **Variance**: ä¸åŒè®­ç»ƒæ•°æ®è®­ç»ƒçš„æ¨¡å‹çš„é¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚ -- æ¨¡å‹è¿‡äºå¤æ‚ï¼Œè¿‡æ‹Ÿåˆ
* **Overfitting**: è¿‡æ‹Ÿåˆ, too many parameters, model predicts the known data well but poor predicts the unknow data. 
Low Bias, High Variance 
1. Increase the sample size: Data Augmentation 
2. Simplified model: Dropout, Early Stopping, Tree Pruning
3. L1, L2 Regularization or increase the penalty factor 
4. Bagging or Bootstrapping 
5. Use cross-validation 

* **Underfitting**: æ¬ æ‹Ÿåˆ, training is worse, model is too simple 
High Bias, low Variance
1. increase feature
2. increase model complexity: increase the number of neurons in NN; increase number of layers; use Kernel function in SVM
3. decrease the regularization factor 

### Q8. Linear Model vs. Non-Linear Model çº¿æ€§æ¨¡å‹ / éçº¿æ€§æ¨¡å‹
* **Linear**: ```Perceptron, Linear SVM, KNN, K-Means Clusering, LSA```
* **Non-Linear**: ```Kernel SVM, AdaBoost, Neuron Network```

### Q9. Generative Approach vs. Discriminative Approach ç”Ÿæˆæ¨¡å‹ / åˆ¤åˆ«æ¨¡å‹
ç›‘ç£å­¦ä¹ çš„æ¨¡å‹ä¸€èˆ¬å½¢å¼ä¸ºå†³ç­–å‡½æ•°(decision function): ```Y=f(x)``` æˆ–æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ(conditional prob. distribution)```P(Y|X)```.     
* **ç”Ÿæˆæ¨¡å‹**: æ•°æ®å­¦ä¹ è”åˆæ¦‚ç‡åˆ†å¸ƒ```P(X,Y)```æ±‚å‡ºæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ```P(Y|X)```ä½œä¸ºé¢„æµ‹æ¨¡å‹:  $$ P(Y|X) = \frac{P(X,Y)}{P(X)} $$    
A model represents a generative relationship given an input X, produces an output Y.    
```æœ´ç´ è´å¶æ–¯(Naive Bayes), éšé©¬å°”ç§‘å¤«æ¨¡å‹(Hidden Markov Model)```
* **åˆ¤åˆ«æ¨¡å‹**: æ•°æ®ç›´æ¥å­¦ä¹ å†³ç­–å‡½æ•°```f(x)```æˆ–è€…æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ```P(Y|X)```ä½œä¸ºé¢„æµ‹æ¨¡å‹    
What output Y should be predicted for a given input X.   
```KNN, Perceptron, Decision Tree, Logistic Regression, SVM, CRF(conditional random field)```

### Q10. Probability æ¦‚ç‡æ¨¡å‹ vs. éæ¦‚ç‡æ¨¡å‹  
* **æ¦‚ç‡æ¨¡å‹**: ä¸€å®šèƒ½è¡¨ç¤ºä¸ºè”åˆæ¦‚ç‡åˆ†å¸ƒ(Joint Prob.Distribution) -- ç”Ÿæˆæ¨¡å‹, ```Decision Tree, Naive Bayes, HMM, CRF, Gaussian Mixture Model```
* **éæ¦‚ç‡æ¨¡å‹**: åˆ¤åˆ«æ¨¡å‹, ```Perceptron, SVM, KNN, AdaBoost, KMeans, Neuron Networks```

### Q11. Parametric å‚æ•°åŒ– vs. éå‚æ•°åŒ–  
* **å‚æ•°åŒ–**: å‡è®¾æ¨¡å‹å‚æ•°ç»´åº¦å›ºå®š(æœ‰é™), Perceptron, Naive Bayes, Logistic Regression, ```KNN, Gaussian Mixture Model```
* **éå‚æ•°åŒ–**: ç»´åº¦ä¸å›ºå®š, éšç€è®­ç»ƒæ•°æ®é‡å¢åŠ è€Œå¢å¤§, ```Decision Tree, SVM, AdaBoost, KMeans```

### Q12. Supervised Learning vs. Unsupervised Learning vs. Reinforcement Learning 
### ç›‘ç£å­¦ä¹  vs éç›‘ç£å­¦ä¹  vs å¼ºåŒ–å­¦ä¹ 
* **Supervised**: known and labeled data, has feedback mechanism. ```Decision Tree, Logistic Regression, SVM```
* **Unsupervised**: unlabeled data, ```K-means clustering, PCA, ICA```
* **Reinforcement**: reward from environment ```Game Theory, Cybernetics, Operations Research, information theory```

### Q13. Univariate(å•å˜é‡), bivariate(åŒå˜é‡) and multivariate(å¤šå˜é‡) analysis 
* **Univariate**: one variable, describe the data and find the patterns that exist within it. 
* **Bivariate**: two variables, deals with causes and relationships between these 2 variables
* **Multivarite**: categorized under multivariate, with more than one dependent variable. 

### Q14. Feature Selection Methods 
* **Filter Methods**: ```LDA(Linear Discrimination Analysis), ANOVA(Analysis of Variance), Chi-Square```
* **Wrapper Methods**: ```Forward Selection, Backward Selection, Recursive Feature Elimination```

### Q15. Dimensionality reduction é™ç»´ / curse of dimensionality
Dimensionality reduction refers to the process of converting a vast dimension dataset into data with fewer dimensions to convey similar information.     
This reduction helps in compressing data and reducing storage space, less computing, removes redundant features. 
* Manual Feature Selection æ‰‹åŠ¨ç­›é€‰ç‰¹å¾
* Principal Component Analysis (PCA)
* Multidimensional Scaling (å¤šç»´åº¦æ ‡)
* Locally linear embedding 

### Q16. Gradient Descent æ¢¯åº¦ä¸‹é™
æ˜¯è¿­ä»£æ³•(Iterative Method)çš„ä¸€ç§, å¯ä»¥ç”¨äºè§£å†³æœ€å°äºŒä¹˜é—®é¢˜(Least Square Problem), å¯æ±‚è§£MLçš„æ¨¡å‹å‚æ•°.    
åœ¨æ±‚è§£æŸå¤±å‡½æ•°çš„æœ€å°å€¼æ—¶é€šè¿‡æ¢¯åº¦ä¸‹é™æ¥è¿­ä»£æ±‚è§£, å¾—åˆ°æœ€å°åŒ–çš„æŸå¤±å‡½æ•°å’Œæ¨¡å‹å‚æ•°.    
```éšæœºæ¢¯åº¦ä¸‹é™(stochastic gradient descentï¼ŒSGD) & æ‰¹é‡æ¢¯åº¦ä¸‹é™(Batch GD)```   

It is an **optimization algorithm** used to find the values of parameters of a function that minimizes a **cost** function.   
It is best used when the parameters cannot be calculated analytically(eg: using linear algebra) and must be searched for by an optimizer algorithm. 

### Q17. Regularization æ­£åˆ™åŒ–
A technique that discourage learning a more complex or flexible model, to avoid the risk of overfitting.   
é˜²æ­¢è¿‡æ‹Ÿåˆ, åœ¨æŸå¤±å‡½æ•°ä¸ŠåŠ ä¸ŠæŸäº›è§„åˆ™(é™åˆ¶), ç¼©å°è§£ç©ºé—´, ä»è€Œå‡å°‘æ±‚å‡ºæ‹Ÿåˆè§£çš„å¯èƒ½æ€§    
* **L1/Lasso**: ç»å¯¹å€¼, the L1 penalty can force the coefficient estimates to be 0. Performs variavle selection
* **L2/Ridge**: æœ€å°äºŒä¹˜, the coefficients is very close to 0 but never equal to 0. == include all the parameters 

### Q18. Normalization å½’ä¸€åŒ–
A very important **preprocessing** step, used to **rescale** values to fit in a specific range to assure better convergence during ackpropagation.    
The data normalization makes all features weighted equally.    
ç”¨äºé‡æ–°è°ƒæ•´æ•°å€¼ä»¥é€‚åº”ç‰¹å®šèŒƒå›´ï¼Œç¡®ä¿åœ¨åå‘ä¼ æ’­æœŸé—´èƒ½æ›´å¥½çš„æ”¶æ•›. æ•°æ®å½’ä¸€åŒ–ä½¿å¾—æ‰€æœ‰ç‰¹å¾æƒé‡ç›¸ç­‰

### Q19. PCA ä¸»æˆåˆ†åˆ†æ
åˆ©ç”¨æ­£äº¤å˜æ¢å¯¹ä¸€ç³»åˆ—å¯èƒ½ç›¸å…³çš„å˜é‡çš„è§‚æµ‹å€¼è¿›è¡Œçº¿æ€§å˜æ¢, ä»è€ŒæŠ•å½±ä¸ºä¸€ç³»åˆ—çº¿æ€§ä¸ç›¸å…³å˜é‡çš„å€¼, å³ä¸ºä¸»æˆåˆ†ã€‚    
* A sequence of p unit vectors, where the i-th vector is the direction of a line that best fits the data while being orthogonal to the first i-1 vectors.
* Different individual dimensions of the data are linear uncorrelated. 
* When many of the variables are highly correlated, and PCA is used to reduce their number to an independent set. 

### Q20. Distance
* **L1 Distance/Manhattan**: ```d(x, y) = |x1 - x2| + |y1 - y2|```
* **L2 Distance/Euclidiean**: ```d(x, y) = sqrt((x1 - y1)^2 + (x2 - y3)^2 + ... + (xn - yn)^2)```

### Q21. How should you maintain a deployed model?
Monitor + Evaluate + Compare + Rebuild.

### Q22. KNN vs. K-Means CLustering 
both need to calculate the distance between samples 
* **KNN**: K Nearest Neighbour, supervised learning, need labeled data, ç»Ÿè®¡å®ƒé™„è¿‘æœ€è¿‘çš„Kä¸ªæ ·æœ¬å¹¶å°†å…¶åˆ’åˆ†åˆ°æœ€å¤šçš„ç±»åˆ«ä¸­
* **K-Means**: unsupervised learning, å°†æ•°æ®åˆ†æˆKç»„ï¼Œéšæœºé€‰æ‹©Kä¸ªå¯¹è±¡ä½œä¸ºåˆå§‹èšç±»çš„ä¸­å¿ƒï¼Œç„¶åè®¡ç®—æ¯ä¸ªå¯¹è±¡ä¸å„ä¸ªä¸­å¿ƒä¹‹é—´çš„è·ç¦»ï¼Œå°†å®ƒåˆ†å…¥æœ€è¿‘çš„èšç±»ä¸­ã€‚



